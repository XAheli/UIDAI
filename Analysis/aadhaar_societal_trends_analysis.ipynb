{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlocking Societal Trends in Aadhaar Enrolment and Updates\n",
    "\n",
    "## UIDAI Hackathon 2025 - Comprehensive Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Problem Statement and Approach\n",
    "\n",
    "### 1.1 Problem Context\n",
    "\n",
    "Aadhaar, India's unique identification system, serves over 1.4 billion residents. Understanding enrollment and update patterns across demographics and geographies is crucial for:\n",
    "\n",
    "- **Resource Allocation**: Optimizing enrollment center distribution\n",
    "- **Policy Planning**: Identifying underserved populations\n",
    "- **Operational Efficiency**: Predicting demand surges\n",
    "- **Quality Assurance**: Detecting anomalies in enrollment patterns\n",
    "\n",
    "### 1.2 Research Questions\n",
    "\n",
    "1. **Temporal Patterns**: What are the daily, weekly, and monthly enrollment trends?\n",
    "2. **Geographic Disparities**: Which states/districts show enrollment gaps?\n",
    "3. **Demographic Distribution**: How do age groups differ in enrollment behavior?\n",
    "4. **Biometric vs Demographic**: What's the relationship between biometric and demographic updates?\n",
    "5. **Anomaly Detection**: Are there unusual patterns indicating operational issues?\n",
    "6. **Predictive Indicators**: Can we forecast future enrollment demand?\n",
    "\n",
    "### 1.3 Analytical Approach\n",
    "\n",
    "```\n",
    "Data Loading → Cleaning → EDA (Uni/Bi/Trivariate) → Pattern Discovery → Anomaly Detection → Prediction → Insights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Datasets Used\n",
    "\n",
    "### 2.1 Dataset Overview\n",
    "\n",
    "| Dataset | Records | Size | Description |\n",
    "|---------|---------|------|-------------|\n",
    "| **Biometric** | 1.86M | 79 MB | Biometric update data by age group |\n",
    "| **Demographic** | 2.07M | 88 MB | Demographic update data by age group |\n",
    "| **Enrollment** | 1.0M | 44 MB | New enrollment data with 3 age groups |\n",
    "\n",
    "### 2.2 Column Descriptions\n",
    "\n",
    "**Common Columns (All Datasets):**\n",
    "- `date`: Date of record (DD-MM-YYYY)\n",
    "- `state`: Indian state/union territory\n",
    "- `district`: District within state\n",
    "- `pincode`: 6-digit postal code\n",
    "\n",
    "**Biometric Dataset:**\n",
    "- `bio_age_5_17`: Biometric updates for ages 5-17\n",
    "- `bio_age_17_`: Biometric updates for ages 17+\n",
    "\n",
    "**Demographic Dataset:**\n",
    "- `demo_age_5_17`: Demographic updates for ages 5-17\n",
    "- `demo_age_17_`: Demographic updates for ages 17+\n",
    "\n",
    "**Enrollment Dataset:**\n",
    "- `age_0_5`: New enrollments for ages 0-5\n",
    "- `age_5_17`: New enrollments for ages 5-17\n",
    "- `age_18_greater`: New enrollments for ages 18+\n",
    "\n",
    "### 2.3 Data Coverage\n",
    "- **Temporal**: March 1, 2025 - December 31, 2025 (10 months)\n",
    "- **Geographic**: 36 States/UTs, 900+ Districts, 19,700+ Pincodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Methodology\n",
    "\n",
    "### 3.1 Data Preprocessing Pipeline\n",
    "\n",
    "1. **Data Loading**: Concatenate chunked CSV files\n",
    "2. **State Standardization**: Map variations to official names\n",
    "3. **District Cleaning**: Remove special characters, normalize names\n",
    "4. **Date Parsing**: Convert to datetime, extract temporal features\n",
    "5. **Feature Engineering**: Create derived metrics for analysis\n",
    "\n",
    "### 3.2 Analysis Framework\n",
    "\n",
    "- **Univariate**: Distribution analysis of individual variables\n",
    "- **Bivariate**: Correlation and relationship analysis\n",
    "- **Trivariate**: Multi-dimensional pattern discovery\n",
    "- **Time Series**: Trend and seasonality decomposition\n",
    "- **Statistical**: Outlier detection using IQR and Z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 3.3: SETUP AND LIBRARY IMPORTS\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 3.4: DATA LOADING\n",
    "# ============================================\n",
    "\n",
    "BASE_PATH = '../Dataset_aheli/'\n",
    "\n",
    "def load_dataset(folder_name):\n",
    "    \"\"\"Load and concatenate all CSV files from a folder.\"\"\"\n",
    "    path = os.path.join(BASE_PATH, folder_name)\n",
    "    files = sorted(glob.glob(os.path.join(path, '*.csv')))\n",
    "    \n",
    "    print(f\"Loading {len(files)} files from {folder_name}...\")\n",
    "    \n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        dfs.append(df)\n",
    "        print(f\"  - {os.path.basename(f)}: {len(df):,} rows\")\n",
    "    \n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"  Total: {len(combined):,} rows\\n\")\n",
    "    return combined\n",
    "\n",
    "# Load all three datasets\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "df_bio = load_dataset('api_data_aadhar_biometric')\n",
    "df_demo = load_dataset('api_data_aadhar_demographic')\n",
    "df_enroll = load_dataset('api_data_aadhar_enrolment')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LOADING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 3.5: INITIAL DATA INSPECTION\n",
    "# ============================================\n",
    "\n",
    "print(\"BIOMETRIC DATA\")\n",
    "print(\"-\" * 40)\n",
    "display(df_bio.info())\n",
    "display(df_bio.head())\n",
    "\n",
    "print(\"\\nDEMOGRAPHIC DATA\")\n",
    "print(\"-\" * 40)\n",
    "display(df_demo.info())\n",
    "display(df_demo.head())\n",
    "\n",
    "print(\"\\nENROLLMENT DATA\")\n",
    "print(\"-\" * 40)\n",
    "display(df_enroll.info())\n",
    "display(df_enroll.head())"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# SECTION 3.6: STATE NAME STANDARDIZATION\n# ============================================\n\n# Comprehensive state name mapping dictionary\nSTATE_MAPPING = {\n    # Andhra Pradesh variations\n    'andhra pradesh': 'Andhra Pradesh',\n    'Andhra pradesh': 'Andhra Pradesh',\n    'ANDHRA PRADESH': 'Andhra Pradesh',\n    \n    # Arunachal Pradesh\n    'arunachal pradesh': 'Arunachal Pradesh',\n    'Arunachal pradesh': 'Arunachal Pradesh',\n    \n    # Assam\n    'assam': 'Assam',\n    'ASSAM': 'Assam',\n    \n    # Bihar\n    'bihar': 'Bihar',\n    'BIHAR': 'Bihar',\n    \n    # Chhattisgarh - common misspelling\n    'Chhatisgarh': 'Chhattisgarh',\n    'chhatisgarh': 'Chhattisgarh',\n    'Chattisgarh': 'Chhattisgarh',\n    'CHHATTISGARH': 'Chhattisgarh',\n    \n    # Goa\n    'goa': 'Goa',\n    'GOA': 'Goa',\n    \n    # Gujarat\n    'gujarat': 'Gujarat',\n    'GUJARAT': 'Gujarat',\n    \n    # Haryana\n    'haryana': 'Haryana',\n    'HARYANA': 'Haryana',\n    \n    # Himachal Pradesh\n    'himachal pradesh': 'Himachal Pradesh',\n    'Himachal pradesh': 'Himachal Pradesh',\n    \n    # Jharkhand\n    'jharkhand': 'Jharkhand',\n    'JHARKHAND': 'Jharkhand',\n    \n    # Karnataka\n    'karnataka': 'Karnataka',\n    'KARNATAKA': 'Karnataka',\n    \n    # Kerala\n    'kerala': 'Kerala',\n    'KERALA': 'Kerala',\n    \n    # Madhya Pradesh\n    'madhya pradesh': 'Madhya Pradesh',\n    'Madhya pradesh': 'Madhya Pradesh',\n    \n    # Maharashtra\n    'maharashtra': 'Maharashtra',\n    'MAHARASHTRA': 'Maharashtra',\n    \n    # Manipur\n    'manipur': 'Manipur',\n    'MANIPUR': 'Manipur',\n    \n    # Meghalaya\n    'meghalaya': 'Meghalaya',\n    'MEGHALAYA': 'Meghalaya',\n    \n    # Mizoram\n    'mizoram': 'Mizoram',\n    'MIZORAM': 'Mizoram',\n    \n    # Nagaland\n    'nagaland': 'Nagaland',\n    'NAGALAND': 'Nagaland',\n    \n    # Odisha - old name mapping\n    'Orissa': 'Odisha',\n    'orissa': 'Odisha',\n    'ORISSA': 'Odisha',\n    'odisha': 'Odisha',\n    'ODISHA': 'Odisha',\n    \n    # Punjab\n    'punjab': 'Punjab',\n    'PUNJAB': 'Punjab',\n    \n    # Rajasthan\n    'rajasthan': 'Rajasthan',\n    'RAJASTHAN': 'Rajasthan',\n    \n    # Sikkim\n    'sikkim': 'Sikkim',\n    'SIKKIM': 'Sikkim',\n    \n    # Tamil Nadu\n    'tamil nadu': 'Tamil Nadu',\n    'Tamil nadu': 'Tamil Nadu',\n    'Tamilnadu': 'Tamil Nadu',\n    'TAMIL NADU': 'Tamil Nadu',\n    \n    # Telangana\n    'telangana': 'Telangana',\n    'TELANGANA': 'Telangana',\n    \n    # Tripura\n    'tripura': 'Tripura',\n    'TRIPURA': 'Tripura',\n    \n    # Uttar Pradesh\n    'uttar pradesh': 'Uttar Pradesh',\n    'Uttar pradesh': 'Uttar Pradesh',\n    'UTTAR PRADESH': 'Uttar Pradesh',\n    \n    # Uttarakhand - old name mapping\n    'Uttaranchal': 'Uttarakhand',\n    'uttaranchal': 'Uttarakhand',\n    'uttarakhand': 'Uttarakhand',\n    'UTTARAKHAND': 'Uttarakhand',\n    \n    # West Bengal\n    'west bengal': 'West Bengal',\n    'West bengal': 'West Bengal',\n    'Westbengal': 'West Bengal',\n    'WEST BENGAL': 'West Bengal',\n    \n    # Union Territories\n    'Andaman and Nicobar Islands': 'Andaman and Nicobar Islands',\n    'Andaman & Nicobar Islands': 'Andaman and Nicobar Islands',\n    'andaman and nicobar islands': 'Andaman and Nicobar Islands',\n    'A & N Islands': 'Andaman and Nicobar Islands',\n    \n    'Chandigarh': 'Chandigarh',\n    'chandigarh': 'Chandigarh',\n    \n    'Dadra and Nagar Haveli and Daman and Diu': 'Dadra and Nagar Haveli and Daman and Diu',\n    'Dadra & Nagar Haveli': 'Dadra and Nagar Haveli and Daman and Diu',\n    'Dadra and Nagar Haveli': 'Dadra and Nagar Haveli and Daman and Diu',\n    'Daman and Diu': 'Dadra and Nagar Haveli and Daman and Diu',\n    'Daman & Diu': 'Dadra and Nagar Haveli and Daman and Diu',\n    'DNH and DD': 'Dadra and Nagar Haveli and Daman and Diu',\n    \n    'Delhi': 'Delhi',\n    'delhi': 'Delhi',\n    'NCT of Delhi': 'Delhi',\n    'NCT Delhi': 'Delhi',\n    \n    'Goa': 'Goa',\n    \n    'Jammu and Kashmir': 'Jammu and Kashmir',\n    'Jammu & Kashmir': 'Jammu and Kashmir',\n    'jammu and kashmir': 'Jammu and Kashmir',\n    'J&K': 'Jammu and Kashmir',\n    \n    'Ladakh': 'Ladakh',\n    'ladakh': 'Ladakh',\n    \n    'Lakshadweep': 'Lakshadweep',\n    'lakshadweep': 'Lakshadweep',\n    \n    'Puducherry': 'Puducherry',\n    'Pondicherry': 'Puducherry',\n    'pondicherry': 'Puducherry',\n    'puducherry': 'Puducherry',\n}\n\ndef standardize_state(state):\n    \"\"\"Standardize state name using mapping dictionary.\"\"\"\n    if pd.isna(state):\n        return state\n    state_str = str(state).strip()\n    return STATE_MAPPING.get(state_str, state_str)\n\n# Apply state standardization\nprint(\"Standardizing state names...\")\ndf_bio['state'] = df_bio['state'].apply(standardize_state)\ndf_demo['state'] = df_demo['state'].apply(standardize_state)\ndf_enroll['state'] = df_enroll['state'].apply(standardize_state)\n\nprint(f\"\\nUnique states after standardization:\")\nprint(f\"  Biometric: {df_bio['state'].nunique()}\")\nprint(f\"  Demographic: {df_demo['state'].nunique()}\")\nprint(f\"  Enrollment: {df_enroll['state'].nunique()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# SECTION 3.7: DISTRICT NAME CLEANING\n# ============================================\n\nimport re\n\ndef clean_district(district):\n    \"\"\"Clean district names by removing special characters and normalizing.\"\"\"\n    if pd.isna(district):\n        return district\n    district_str = str(district).strip()\n    # Remove asterisks and extra spaces\n    district_str = re.sub(r'\\s*\\*\\s*', '', district_str)\n    # Remove special characters except spaces and hyphens\n    district_str = re.sub(r'[^a-zA-Z\\s\\-]', '', district_str)\n    # Normalize multiple spaces\n    district_str = re.sub(r'\\s+', ' ', district_str)\n    # Title case\n    district_str = district_str.strip().title()\n    return district_str\n\n# Apply district cleaning\nprint(\"Cleaning district names...\")\ndf_bio['district'] = df_bio['district'].apply(clean_district)\ndf_demo['district'] = df_demo['district'].apply(clean_district)\ndf_enroll['district'] = df_enroll['district'].apply(clean_district)\n\nprint(f\"\\nUnique districts after cleaning:\")\nprint(f\"  Biometric: {df_bio['district'].nunique()}\")\nprint(f\"  Demographic: {df_demo['district'].nunique()}\")\nprint(f\"  Enrollment: {df_enroll['district'].nunique()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# SECTION 3.8: DATE PARSING AND TEMPORAL FEATURES\n# ============================================\n\ndef parse_and_extract_features(df, date_col='date'):\n    \"\"\"Parse dates and extract temporal features.\"\"\"\n    # Parse date\n    df['date'] = pd.to_datetime(df[date_col], format='%d-%m-%Y', errors='coerce')\n    \n    # Extract temporal features\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['month_name'] = df['date'].dt.month_name()\n    df['week'] = df['date'].dt.isocalendar().week\n    df['day'] = df['date'].dt.day\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['day_name'] = df['date'].dt.day_name()\n    df['quarter'] = df['date'].dt.quarter\n    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n    \n    return df\n\n# Apply to all datasets\nprint(\"Parsing dates and extracting temporal features...\")\ndf_bio = parse_and_extract_features(df_bio)\ndf_demo = parse_and_extract_features(df_demo)\ndf_enroll = parse_and_extract_features(df_enroll)\n\nprint(\"\\nDate ranges:\")\nprint(f\"  Biometric: {df_bio['date'].min()} to {df_bio['date'].max()}\")\nprint(f\"  Demographic: {df_demo['date'].min()} to {df_demo['date'].max()}\")\nprint(f\"  Enrollment: {df_enroll['date'].min()} to {df_enroll['date'].max()}\")\n\n# Display sample with new features\nprint(\"\\nSample with temporal features (Enrollment):\")\ndisplay(df_enroll[['date', 'year', 'month', 'month_name', 'day_name', 'quarter', 'is_weekend']].head(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# SECTION 3.9: CREATE TOTAL COLUMNS AND SUMMARY\n# ============================================\n\n# Add total columns for easier analysis\ndf_bio['total_bio'] = df_bio['bio_age_5_17'] + df_bio['bio_age_17_']\ndf_demo['total_demo'] = df_demo['demo_age_5_17'] + df_demo['demo_age_17_']\ndf_enroll['total_enroll'] = df_enroll['age_0_5'] + df_enroll['age_5_17'] + df_enroll['age_18_greater']\n\n# Summary statistics\nprint(\"=\"*60)\nprint(\"DATA SUMMARY AFTER PREPROCESSING\")\nprint(\"=\"*60)\n\nprint(\"\\n--- BIOMETRIC DATA ---\")\nprint(f\"Total Records: {len(df_bio):,}\")\nprint(f\"Total Biometric Updates: {df_bio['total_bio'].sum():,}\")\nprint(f\"Date Range: {df_bio['date'].min().strftime('%Y-%m-%d')} to {df_bio['date'].max().strftime('%Y-%m-%d')}\")\nprint(f\"Unique States: {df_bio['state'].nunique()}\")\nprint(f\"Unique Districts: {df_bio['district'].nunique()}\")\nprint(f\"Unique Pincodes: {df_bio['pincode'].nunique()}\")\n\nprint(\"\\n--- DEMOGRAPHIC DATA ---\")\nprint(f\"Total Records: {len(df_demo):,}\")\nprint(f\"Total Demographic Updates: {df_demo['total_demo'].sum():,}\")\nprint(f\"Date Range: {df_demo['date'].min().strftime('%Y-%m-%d')} to {df_demo['date'].max().strftime('%Y-%m-%d')}\")\nprint(f\"Unique States: {df_demo['state'].nunique()}\")\nprint(f\"Unique Districts: {df_demo['district'].nunique()}\")\nprint(f\"Unique Pincodes: {df_demo['pincode'].nunique()}\")\n\nprint(\"\\n--- ENROLLMENT DATA ---\")\nprint(f\"Total Records: {len(df_enroll):,}\")\nprint(f\"Total New Enrollments: {df_enroll['total_enroll'].sum():,}\")\nprint(f\"Date Range: {df_enroll['date'].min().strftime('%Y-%m-%d')} to {df_enroll['date'].max().strftime('%Y-%m-%d')}\")\nprint(f\"Unique States: {df_enroll['state'].nunique()}\")\nprint(f\"Unique Districts: {df_enroll['district'].nunique()}\")\nprint(f\"Unique Pincodes: {df_enroll['pincode'].nunique()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 4. Data Analysis and Visualization\n\n### 4.1 Univariate Analysis\n\nAnalyzing individual variables to understand their distributions and characteristics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.1.1: STATE-WISE ENROLLMENT DISTRIBUTION\n# ============================================\n\n# Aggregate by state for enrollment data\nstate_enroll = df_enroll.groupby('state').agg({\n    'total_enroll': 'sum',\n    'age_0_5': 'sum',\n    'age_5_17': 'sum',\n    'age_18_greater': 'sum'\n}).reset_index()\nstate_enroll = state_enroll.sort_values('total_enroll', ascending=True)\n\n# Create horizontal bar chart\nfig, ax = plt.subplots(figsize=(14, 12))\ncolors = plt.cm.viridis(np.linspace(0.2, 0.8, len(state_enroll)))\n\nbars = ax.barh(state_enroll['state'], state_enroll['total_enroll'], color=colors)\nax.set_xlabel('Total Enrollments', fontsize=12)\nax.set_ylabel('State/UT', fontsize=12)\nax.set_title('State-wise Aadhaar Enrollment Distribution (March-December 2025)', fontsize=14, fontweight='bold')\n\n# Add value labels\nfor bar, val in zip(bars, state_enroll['total_enroll']):\n    ax.text(val + state_enroll['total_enroll'].max()*0.01, bar.get_y() + bar.get_height()/2, \n            f'{val:,.0f}', va='center', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n# Print top 10 states\nprint(\"\\nTop 10 States by Total Enrollment:\")\nprint(state_enroll.nlargest(10, 'total_enroll')[['state', 'total_enroll']].to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.1.2: AGE GROUP DISTRIBUTION (PIE CHART)\n# ============================================\n\n# Calculate total enrollments by age group\nage_totals = {\n    'Age 0-5 (Infants)': df_enroll['age_0_5'].sum(),\n    'Age 5-17 (Children/Youth)': df_enroll['age_5_17'].sum(),\n    'Age 18+ (Adults)': df_enroll['age_18_greater'].sum()\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Enrollment Age Distribution\ncolors_enroll = ['#ff9999', '#66b3ff', '#99ff99']\naxes[0].pie(age_totals.values(), labels=age_totals.keys(), autopct='%1.1f%%', \n            colors=colors_enroll, explode=(0.05, 0.05, 0.05), shadow=True)\naxes[0].set_title('Enrollment by Age Group', fontsize=14, fontweight='bold')\n\n# Biometric Age Distribution\nbio_totals = {\n    'Age 5-17': df_bio['bio_age_5_17'].sum(),\n    'Age 17+': df_bio['bio_age_17_'].sum()\n}\ncolors_bio = ['#ffcc99', '#c2c2f0']\naxes[1].pie(bio_totals.values(), labels=bio_totals.keys(), autopct='%1.1f%%',\n            colors=colors_bio, explode=(0.05, 0.05), shadow=True)\naxes[1].set_title('Biometric Updates by Age Group', fontsize=14, fontweight='bold')\n\n# Demographic Age Distribution\ndemo_totals = {\n    'Age 5-17': df_demo['demo_age_5_17'].sum(),\n    'Age 17+': df_demo['demo_age_17_'].sum()\n}\ncolors_demo = ['#ffb3e6', '#c4e17f']\naxes[2].pie(demo_totals.values(), labels=demo_totals.keys(), autopct='%1.1f%%',\n            colors=colors_demo, explode=(0.05, 0.05), shadow=True)\naxes[2].set_title('Demographic Updates by Age Group', fontsize=14, fontweight='bold')\n\nplt.suptitle('Age Group Distribution Across All Datasets', fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Print statistics\nprint(\"\\n--- AGE GROUP STATISTICS ---\")\nprint(f\"\\nEnrollment Totals:\")\nfor age, count in age_totals.items():\n    print(f\"  {age}: {count:,}\")\nprint(f\"\\nBiometric Updates: 5-17: {bio_totals['Age 5-17']:,} | 17+: {bio_totals['Age 17+']:,}\")\nprint(f\"Demographic Updates: 5-17: {demo_totals['Age 5-17']:,} | 17+: {demo_totals['Age 17+']:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.1.3: MONTHLY ENROLLMENT TRENDS\n# ============================================\n\n# Aggregate by month\nmonth_order = ['March', 'April', 'May', 'June', 'July', 'August', \n               'September', 'October', 'November', 'December']\n\nmonthly_enroll = df_enroll.groupby('month_name').agg({\n    'total_enroll': 'sum',\n    'age_0_5': 'sum',\n    'age_5_17': 'sum',\n    'age_18_greater': 'sum'\n}).reindex(month_order)\n\nmonthly_bio = df_bio.groupby('month_name')['total_bio'].sum().reindex(month_order)\nmonthly_demo = df_demo.groupby('month_name')['total_demo'].sum().reindex(month_order)\n\n# Create figure with multiple subplots\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Plot 1: Monthly enrollment trends\nax1 = axes[0, 0]\nax1.plot(monthly_enroll.index, monthly_enroll['total_enroll'], marker='o', linewidth=2, \n         markersize=8, color='#2ecc71', label='Total Enrollment')\nax1.fill_between(monthly_enroll.index, monthly_enroll['total_enroll'], alpha=0.3, color='#2ecc71')\nax1.set_title('Monthly Enrollment Trend', fontsize=14, fontweight='bold')\nax1.set_xlabel('Month')\nax1.set_ylabel('Total Enrollments')\nax1.tick_params(axis='x', rotation=45)\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Biometric vs Demographic monthly\nax2 = axes[0, 1]\nx = np.arange(len(month_order))\nwidth = 0.35\nax2.bar(x - width/2, monthly_bio.values, width, label='Biometric', color='#3498db')\nax2.bar(x + width/2, monthly_demo.values, width, label='Demographic', color='#e74c3c')\nax2.set_title('Biometric vs Demographic Updates by Month', fontsize=14, fontweight='bold')\nax2.set_xlabel('Month')\nax2.set_ylabel('Total Updates')\nax2.set_xticks(x)\nax2.set_xticklabels(month_order, rotation=45)\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\n\n# Plot 3: Age-wise monthly enrollment stacked\nax3 = axes[1, 0]\nax3.bar(monthly_enroll.index, monthly_enroll['age_0_5'], label='Age 0-5', color='#ff9999')\nax3.bar(monthly_enroll.index, monthly_enroll['age_5_17'], bottom=monthly_enroll['age_0_5'], \n        label='Age 5-17', color='#66b3ff')\nax3.bar(monthly_enroll.index, monthly_enroll['age_18_greater'], \n        bottom=monthly_enroll['age_0_5'] + monthly_enroll['age_5_17'], \n        label='Age 18+', color='#99ff99')\nax3.set_title('Age-wise Monthly Enrollment (Stacked)', fontsize=14, fontweight='bold')\nax3.set_xlabel('Month')\nax3.set_ylabel('Enrollments')\nax3.tick_params(axis='x', rotation=45)\nax3.legend()\nax3.grid(True, alpha=0.3, axis='y')\n\n# Plot 4: All three datasets comparison\nax4 = axes[1, 1]\nax4.plot(month_order, monthly_enroll['total_enroll'].values, marker='s', linewidth=2, \n         label='Enrollment', color='#2ecc71')\nax4.plot(month_order, monthly_bio.values, marker='o', linewidth=2, \n         label='Biometric', color='#3498db')\nax4.plot(month_order, monthly_demo.values, marker='^', linewidth=2, \n         label='Demographic', color='#e74c3c')\nax4.set_title('All Datasets: Monthly Comparison', fontsize=14, fontweight='bold')\nax4.set_xlabel('Month')\nax4.set_ylabel('Total Count')\nax4.tick_params(axis='x', rotation=45)\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print monthly statistics\nprint(\"\\n--- MONTHLY STATISTICS ---\")\nprint(\"\\nEnrollment by Month:\")\nprint(monthly_enroll['total_enroll'].to_string())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.1.4: DAY OF WEEK PATTERNS (WEEKDAY EFFECT)\n# ============================================\n\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n# Aggregate by day of week\ndaily_enroll = df_enroll.groupby('day_name')['total_enroll'].sum().reindex(day_order)\ndaily_bio = df_bio.groupby('day_name')['total_bio'].sum().reindex(day_order)\ndaily_demo = df_demo.groupby('day_name')['total_demo'].sum().reindex(day_order)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Box plot for enrollment distribution by day\nax1 = axes[0]\ndf_daily_agg = df_enroll.groupby(['date', 'day_name'])['total_enroll'].sum().reset_index()\nsns.boxplot(data=df_daily_agg, x='day_name', y='total_enroll', order=day_order, ax=ax1, palette='Set2')\nax1.set_title('Daily Enrollment Distribution by Day of Week', fontsize=14, fontweight='bold')\nax1.set_xlabel('Day of Week')\nax1.set_ylabel('Daily Total Enrollment')\nax1.tick_params(axis='x', rotation=45)\n\n# Bar chart comparison\nax2 = axes[1]\nx = np.arange(len(day_order))\nwidth = 0.25\nax2.bar(x - width, daily_enroll.values/1e6, width, label='Enrollment', color='#2ecc71')\nax2.bar(x, daily_bio.values/1e6, width, label='Biometric', color='#3498db')\nax2.bar(x + width, daily_demo.values/1e6, width, label='Demographic', color='#e74c3c')\nax2.set_title('Day-wise Activity Comparison (in Millions)', fontsize=14, fontweight='bold')\nax2.set_xlabel('Day of Week')\nax2.set_ylabel('Total Count (Millions)')\nax2.set_xticks(x)\nax2.set_xticklabels(day_order, rotation=45)\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Weekend vs Weekday analysis\nweekend_enroll = df_enroll[df_enroll['is_weekend']==1]['total_enroll'].sum()\nweekday_enroll = df_enroll[df_enroll['is_weekend']==0]['total_enroll'].sum()\nprint(\"\\n--- WEEKDAY vs WEEKEND ANALYSIS ---\")\nprint(f\"Weekday Enrollments: {weekday_enroll:,} ({weekday_enroll/(weekday_enroll+weekend_enroll)*100:.1f}%)\")\nprint(f\"Weekend Enrollments: {weekend_enroll:,} ({weekend_enroll/(weekday_enroll+weekend_enroll)*100:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 Bivariate Analysis\n\nAnalyzing relationships between two variables to identify correlations and patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.2.1: STATE x MONTH HEATMAP (ENROLLMENT)\n# ============================================\n\n# Create pivot table for state-month enrollment\nstate_month_pivot = df_enroll.pivot_table(\n    values='total_enroll', \n    index='state', \n    columns='month_name', \n    aggfunc='sum'\n).reindex(columns=month_order)\n\n# Select top 20 states for better visualization\ntop_states = state_enroll.nlargest(20, 'total_enroll')['state'].tolist()\nstate_month_top = state_month_pivot.loc[top_states]\n\nfig, ax = plt.subplots(figsize=(14, 10))\nsns.heatmap(state_month_top, annot=True, fmt='.0f', cmap='YlOrRd', \n            linewidths=0.5, ax=ax, cbar_kws={'label': 'Enrollments'})\nax.set_title('State x Month Enrollment Heatmap (Top 20 States)', fontsize=14, fontweight='bold')\nax.set_xlabel('Month', fontsize=12)\nax.set_ylabel('State', fontsize=12)\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n# Identify peak months per state\nprint(\"\\n--- PEAK ENROLLMENT MONTHS BY STATE (Top 10) ---\")\nfor state in top_states[:10]:\n    peak_month = state_month_pivot.loc[state].idxmax()\n    peak_value = state_month_pivot.loc[state].max()\n    print(f\"  {state}: {peak_month} ({peak_value:,.0f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.2.2: CORRELATION ANALYSIS - AGE GROUPS\n# ============================================\n\n# Create correlation matrix for enrollment age groups\ncorr_data = df_enroll[['age_0_5', 'age_5_17', 'age_18_greater', 'total_enroll']].copy()\ncorr_data.columns = ['Age 0-5', 'Age 5-17', 'Age 18+', 'Total']\ncorrelation_matrix = corr_data.corr()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Correlation heatmap\nax1 = axes[0]\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            square=True, linewidths=0.5, ax=ax1, vmin=-1, vmax=1,\n            fmt='.3f', annot_kws={'size': 12})\nax1.set_title('Correlation Matrix: Enrollment Age Groups', fontsize=14, fontweight='bold')\n\n# Scatter plot: Age 5-17 vs Age 18+\nax2 = axes[1]\nsample_data = df_enroll.sample(min(10000, len(df_enroll)))\nax2.scatter(sample_data['age_5_17'], sample_data['age_18_greater'], alpha=0.3, s=10, c='#3498db')\nax2.set_xlabel('Age 5-17 Enrollments')\nax2.set_ylabel('Age 18+ Enrollments')\nax2.set_title('Scatter Plot: Youth vs Adult Enrollments', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print correlation insights\nprint(\"\\n--- CORRELATION INSIGHTS ---\")\nprint(f\"Age 0-5 ↔ Age 5-17: {correlation_matrix.loc['Age 0-5', 'Age 5-17']:.3f}\")\nprint(f\"Age 5-17 ↔ Age 18+: {correlation_matrix.loc['Age 5-17', 'Age 18+']:.3f}\")\nprint(f\"Age 0-5 ↔ Age 18+: {correlation_matrix.loc['Age 0-5', 'Age 18+']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 Trivariate Analysis\n\nAnalyzing relationships among three variables to discover multi-dimensional patterns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.3.1: STATE x MONTH x AGE GROUP (FACETED)\n# ============================================\n\n# Select top 6 states for visualization\ntop_6_states = state_enroll.nlargest(6, 'total_enroll')['state'].tolist()\ndf_top6 = df_enroll[df_enroll['state'].isin(top_6_states)].copy()\n\n# Aggregate by state and month\nstate_month_age = df_top6.groupby(['state', 'month_name']).agg({\n    'age_0_5': 'sum',\n    'age_5_17': 'sum',\n    'age_18_greater': 'sum'\n}).reset_index()\n\n# Melt for easier plotting\nstate_month_melted = state_month_age.melt(\n    id_vars=['state', 'month_name'],\n    value_vars=['age_0_5', 'age_5_17', 'age_18_greater'],\n    var_name='age_group',\n    value_name='enrollments'\n)\nstate_month_melted['age_group'] = state_month_melted['age_group'].map({\n    'age_0_5': 'Age 0-5',\n    'age_5_17': 'Age 5-17',\n    'age_18_greater': 'Age 18+'\n})\n\n# Create faceted plot\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.flatten()\n\nfor idx, state in enumerate(top_6_states):\n    ax = axes[idx]\n    state_data = state_month_melted[state_month_melted['state'] == state]\n    \n    # Pivot for stacked bar\n    pivot_data = state_data.pivot(index='month_name', columns='age_group', values='enrollments')\n    pivot_data = pivot_data.reindex(month_order)\n    \n    pivot_data.plot(kind='bar', stacked=True, ax=ax, \n                   color=['#ff9999', '#66b3ff', '#99ff99'], width=0.8)\n    ax.set_title(f'{state}', fontsize=12, fontweight='bold')\n    ax.set_xlabel('')\n    ax.set_ylabel('Enrollments')\n    ax.tick_params(axis='x', rotation=45)\n    ax.legend(loc='upper right', fontsize=8)\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.suptitle('Monthly Age-wise Enrollment Distribution (Top 6 States)', \n             fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n--- TOP 6 STATES ANALYSIS ---\")\nfor state in top_6_states:\n    total = state_enroll[state_enroll['state']==state]['total_enroll'].values[0]\n    print(f\"  {state}: {total:,.0f} total enrollments\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 5. Anomaly Detection & Pattern Discovery\n\nIdentifying unusual patterns, outliers, and trends that may indicate operational issues or significant events.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 5.1: DAILY ENROLLMENT ANOMALY DETECTION\n# ============================================\n\n# Aggregate daily enrollment totals\ndaily_totals = df_enroll.groupby('date')['total_enroll'].sum().reset_index()\ndaily_totals = daily_totals.sort_values('date')\n\n# Calculate statistics for anomaly detection\nmean_daily = daily_totals['total_enroll'].mean()\nstd_daily = daily_totals['total_enroll'].std()\n\n# Define anomaly thresholds (using 2 standard deviations)\nupper_threshold = mean_daily + 2 * std_daily\nlower_threshold = mean_daily - 2 * std_daily\n\n# Identify anomalies\ndaily_totals['is_anomaly'] = (daily_totals['total_enroll'] > upper_threshold) | \\\n                             (daily_totals['total_enroll'] < lower_threshold)\ndaily_totals['anomaly_type'] = np.where(\n    daily_totals['total_enroll'] > upper_threshold, 'High',\n    np.where(daily_totals['total_enroll'] < lower_threshold, 'Low', 'Normal')\n)\n\n# Plot\nfig, ax = plt.subplots(figsize=(16, 6))\n\n# Main line\nax.plot(daily_totals['date'], daily_totals['total_enroll'], \n        linewidth=1, alpha=0.7, color='#3498db', label='Daily Enrollment')\n\n# Highlight anomalies\nhigh_anomalies = daily_totals[daily_totals['anomaly_type'] == 'High']\nlow_anomalies = daily_totals[daily_totals['anomaly_type'] == 'Low']\n\nax.scatter(high_anomalies['date'], high_anomalies['total_enroll'], \n           color='red', s=50, zorder=5, label=f'High Anomalies ({len(high_anomalies)})')\nax.scatter(low_anomalies['date'], low_anomalies['total_enroll'], \n           color='orange', s=50, zorder=5, label=f'Low Anomalies ({len(low_anomalies)})')\n\n# Threshold lines\nax.axhline(y=mean_daily, color='green', linestyle='--', alpha=0.7, label=f'Mean ({mean_daily:,.0f})')\nax.axhline(y=upper_threshold, color='red', linestyle=':', alpha=0.5, label=f'Upper Threshold')\nax.axhline(y=lower_threshold, color='orange', linestyle=':', alpha=0.5, label=f'Lower Threshold')\n\nax.set_xlabel('Date')\nax.set_ylabel('Daily Total Enrollment')\nax.set_title('Daily Enrollment Time Series with Anomaly Detection', fontsize=14, fontweight='bold')\nax.legend(loc='upper right')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Print anomaly summary\nprint(\"\\n--- ANOMALY DETECTION SUMMARY ---\")\nprint(f\"Mean Daily Enrollment: {mean_daily:,.0f}\")\nprint(f\"Standard Deviation: {std_daily:,.0f}\")\nprint(f\"Upper Threshold (μ+2σ): {upper_threshold:,.0f}\")\nprint(f\"Lower Threshold (μ-2σ): {lower_threshold:,.0f}\")\nprint(f\"\\nTotal Anomalies Detected: {daily_totals['is_anomaly'].sum()}\")\nprint(f\"  - High Volume Days: {len(high_anomalies)}\")\nprint(f\"  - Low Volume Days: {len(low_anomalies)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 5.2: TOP DISTRICTS ANALYSIS\n# ============================================\n\n# Aggregate by district\ndistrict_enroll = df_enroll.groupby(['state', 'district']).agg({\n    'total_enroll': 'sum',\n    'age_0_5': 'sum',\n    'age_5_17': 'sum',\n    'age_18_greater': 'sum'\n}).reset_index()\n\n# Top 20 districts\ntop_20_districts = district_enroll.nlargest(20, 'total_enroll')\n\nfig, ax = plt.subplots(figsize=(14, 10))\ndistrict_labels = top_20_districts['district'] + ' (' + top_20_districts['state'].str[:3] + ')'\ncolors = plt.cm.plasma(np.linspace(0.2, 0.8, 20))\n\nbars = ax.barh(range(20), top_20_districts['total_enroll'].values, color=colors)\nax.set_yticks(range(20))\nax.set_yticklabels(district_labels)\nax.invert_yaxis()\nax.set_xlabel('Total Enrollments')\nax.set_title('Top 20 Districts by Enrollment Volume', fontsize=14, fontweight='bold')\n\n# Add value labels\nfor i, (bar, val) in enumerate(zip(bars, top_20_districts['total_enroll'].values)):\n    ax.text(val + top_20_districts['total_enroll'].max()*0.01, i, \n            f'{val:,.0f}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Print top 20 districts\nprint(\"\\n--- TOP 20 DISTRICTS BY ENROLLMENT ---\")\nfor idx, row in top_20_districts.iterrows():\n    print(f\"  {row['district']}, {row['state']}: {row['total_enroll']:,.0f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 6. Key Findings & Recommendations\n\n### Executive Summary of Insights\n\nBased on comprehensive analysis of 5+ million Aadhaar records spanning enrollment, biometric updates, and demographic updates, we present actionable findings and strategic recommendations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 6.1: COMPREHENSIVE KEY FINDINGS\n# ============================================\n\nprint(\"=\"*80)\nprint(\"KEY FINDINGS FROM AADHAAR DATA ANALYSIS\")\nprint(\"=\"*80)\n\n# Calculate key statistics\ntotal_enrollments = df_enroll['total_enroll'].sum()\ntotal_biometric = df_bio['total_bio'].sum()\ntotal_demographic = df_demo['total_demo'].sum()\n\n# Top state\ntop_state = state_enroll.nlargest(1, 'total_enroll').iloc[0]\n\n# Age group analysis\nage_0_5_pct = df_enroll['age_0_5'].sum() / total_enrollments * 100\nage_5_17_pct = df_enroll['age_5_17'].sum() / total_enrollments * 100\nage_18_pct = df_enroll['age_18_greater'].sum() / total_enrollments * 100\n\n# Weekend vs Weekday\nweekend_total = df_enroll[df_enroll['is_weekend']==1]['total_enroll'].sum()\nweekday_total = df_enroll[df_enroll['is_weekend']==0]['total_enroll'].sum()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINDING 1: SCALE OF AADHAAR ACTIVITY\")\nprint(\"=\"*80)\nprint(f\"\"\"\n• Total New Enrollments: {total_enrollments:,}\n• Total Biometric Updates: {total_biometric:,}\n• Total Demographic Updates: {total_demographic:,}\n• Combined Activity: {total_enrollments + total_biometric + total_demographic:,} transactions\n\nINSIGHT: The system handles millions of transactions daily, demonstrating\nrobust infrastructure capable of serving India's 1.4 billion population.\n\"\"\")\n\nprint(\"=\"*80)\nprint(\"FINDING 2: GEOGRAPHIC DISTRIBUTION\")\nprint(\"=\"*80)\nprint(f\"\"\"\n• Highest Enrollment State: {top_state['state']} ({top_state['total_enroll']:,.0f} enrollments)\n• States Covered: {df_enroll['state'].nunique()} states/UTs\n• Districts Covered: {df_enroll['district'].nunique()} districts\n• Pincodes Covered: {df_enroll['pincode'].nunique():,} pincodes\n\nINSIGHT: Significant geographic disparity exists. High-population states\n(UP, Maharashtra, Bihar) dominate enrollment numbers, but per-capita \nanalysis may reveal different efficiency patterns.\n\"\"\")\n\nprint(\"=\"*80)\nprint(\"FINDING 3: AGE GROUP PATTERNS\")\nprint(\"=\"*80)\nprint(f\"\"\"\n• Infant Enrollments (0-5): {age_0_5_pct:.1f}%\n• Child/Youth Enrollments (5-17): {age_5_17_pct:.1f}%\n• Adult Enrollments (18+): {age_18_pct:.1f}%\n\nINSIGHT: Adult enrollments dominate, indicating ongoing enrollment of\npreviously uncovered adults. Child enrollment rates suggest potential\nfor school-based enrollment drives.\n\"\"\")\n\nprint(\"=\"*80)\nprint(\"FINDING 4: TEMPORAL PATTERNS\")\nprint(\"=\"*80)\nprint(f\"\"\"\n• Weekday Enrollments: {weekday_total:,} ({weekday_total/(weekday_total+weekend_total)*100:.1f}%)\n• Weekend Enrollments: {weekend_total:,} ({weekend_total/(weekday_total+weekend_total)*100:.1f}%)\n• Daily Mean: {mean_daily:,.0f}\n• Daily Std Dev: {std_daily:,.0f}\n\nINSIGHT: Strong weekday preference indicates enrollment centers primarily\noperate during business hours. Weekend services could improve accessibility\nfor working populations.\n\"\"\")\n\nprint(\"=\"*80)\nprint(\"FINDING 5: ANOMALY DETECTION RESULTS\")\nprint(\"=\"*80)\nanomaly_count = daily_totals['is_anomaly'].sum()\nprint(f\"\"\"\n• Anomalies Detected: {anomaly_count} days\n• High-Volume Anomalies: {len(high_anomalies)} days\n• Low-Volume Anomalies: {len(low_anomalies)} days\n• Detection Threshold: μ ± 2σ\n\nINSIGHT: Anomalies may correspond to:\n  - High: Year-end rushes, deadline-driven surges, special enrollment camps\n  - Low: Public holidays, natural disasters, system maintenance\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 6.2: STRATEGIC RECOMMENDATIONS\n# ============================================\n\nprint(\"=\"*80)\nprint(\"STRATEGIC RECOMMENDATIONS\")\nprint(\"=\"*80)\n\nprint(\"\"\"\n┌─────────────────────────────────────────────────────────────────────────────┐\n│  RECOMMENDATION 1: OPTIMIZE RESOURCE ALLOCATION                              │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  • Deploy additional enrollment centers in high-demand districts             │\n│  • Implement dynamic staffing based on daily/weekly demand patterns          │\n│  • Use predictive models to anticipate surge periods                         │\n│  • Consider mobile enrollment units for rural/remote areas                   │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│  RECOMMENDATION 2: EXPAND WEEKEND SERVICES                                   │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  • Current weekend activity is significantly lower than weekdays             │\n│  • Expand weekend hours to serve working professionals                       │\n│  • Target Saturday services in commercial/industrial areas                   │\n│  • Measure impact on overall accessibility scores                            │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│  RECOMMENDATION 3: AGE-TARGETED ENROLLMENT DRIVES                            │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  • Partner with hospitals for newborn (0-5) enrollment at birth              │\n│  • Integrate school enrollment (5-17) with academic admissions               │\n│  • Target senior citizens through community outreach programs                │\n│  • Create awareness campaigns for biometric updates among adults             │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│  RECOMMENDATION 4: GEOGRAPHIC EQUITY INITIATIVES                             │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  • Identify underserved districts with low enrollment rates                  │\n│  • Implement special camps in northeastern states and island territories     │\n│  • Incentivize enrollment operators in low-density areas                     │\n│  • Partner with state governments for awareness campaigns                    │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│  RECOMMENDATION 5: ANOMALY MONITORING SYSTEM                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  • Implement real-time anomaly detection dashboards                          │\n│  • Set up automated alerts for significant deviations                        │\n│  • Investigate root causes of low-volume anomalies                           │\n│  • Document and prepare for known high-volume periods                        │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│  RECOMMENDATION 6: DATA QUALITY IMPROVEMENTS                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  • Standardize state/district name entry at source                           │\n│  • Implement validation rules for pincode-state consistency                  │\n│  • Create master data management for geographic hierarchies                  │\n│  • Regular data quality audits and cleansing processes                       │\n└─────────────────────────────────────────────────────────────────────────────┘\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 6.3: FINAL SUMMARY DASHBOARD VISUALIZATION\n# ============================================\n\nfig = plt.figure(figsize=(18, 14))\n\n# Create grid layout\ngs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n\n# 1. Total Activity by Dataset (Top Left)\nax1 = fig.add_subplot(gs[0, 0])\ncategories = ['Enrollment', 'Biometric', 'Demographic']\nvalues = [total_enrollments/1e6, total_biometric/1e6, total_demographic/1e6]\ncolors = ['#2ecc71', '#3498db', '#e74c3c']\nbars = ax1.bar(categories, values, color=colors, edgecolor='black', linewidth=1.5)\nax1.set_ylabel('Transactions (Millions)')\nax1.set_title('Total Activity by Dataset', fontweight='bold')\nfor bar, val in zip(bars, values):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n             f'{val:.1f}M', ha='center', fontweight='bold')\n\n# 2. Age Distribution Donut (Top Center)\nax2 = fig.add_subplot(gs[0, 1])\nage_vals = [df_enroll['age_0_5'].sum(), df_enroll['age_5_17'].sum(), df_enroll['age_18_greater'].sum()]\nage_labels = ['0-5', '5-17', '18+']\nwedges, texts, autotexts = ax2.pie(age_vals, labels=age_labels, autopct='%1.1f%%', \n                                    colors=['#ff9999', '#66b3ff', '#99ff99'],\n                                    wedgeprops=dict(width=0.6), startangle=90)\nax2.set_title('Enrollment by Age Group', fontweight='bold')\n\n# 3. Weekend vs Weekday (Top Right)\nax3 = fig.add_subplot(gs[0, 2])\nww_labels = ['Weekday', 'Weekend']\nww_vals = [weekday_total/1e6, weekend_total/1e6]\nax3.bar(ww_labels, ww_vals, color=['#9b59b6', '#f39c12'], edgecolor='black', linewidth=1.5)\nax3.set_ylabel('Enrollments (Millions)')\nax3.set_title('Weekday vs Weekend', fontweight='bold')\nfor i, v in enumerate(ww_vals):\n    ax3.text(i, v + 0.2, f'{v:.1f}M', ha='center', fontweight='bold')\n\n# 4. Top 10 States (Middle Left - Spanning 2 columns)\nax4 = fig.add_subplot(gs[1, :2])\ntop10 = state_enroll.nlargest(10, 'total_enroll')\nax4.barh(top10['state'], top10['total_enroll']/1e6, color=plt.cm.viridis(np.linspace(0.3, 0.9, 10)))\nax4.set_xlabel('Enrollments (Millions)')\nax4.set_title('Top 10 States by Enrollment', fontweight='bold')\nax4.invert_yaxis()\nfor i, v in enumerate(top10['total_enroll'].values/1e6):\n    ax4.text(v + 0.1, i, f'{v:.2f}M', va='center', fontsize=9)\n\n# 5. Monthly Trend (Middle Right)\nax5 = fig.add_subplot(gs[1, 2])\nmonthly_vals = monthly_enroll['total_enroll'].values/1e6\nmonths_short = ['Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nax5.plot(months_short, monthly_vals, marker='o', linewidth=2, color='#2ecc71', markersize=8)\nax5.fill_between(months_short, monthly_vals, alpha=0.3, color='#2ecc71')\nax5.set_ylabel('Enrollments (Millions)')\nax5.set_title('Monthly Trend', fontweight='bold')\nax5.tick_params(axis='x', rotation=45)\nax5.grid(True, alpha=0.3)\n\n# 6. Key Metrics Summary (Bottom - Spanning all columns)\nax6 = fig.add_subplot(gs[2, :])\nax6.axis('off')\n\n# Create summary text\nsummary_text = f\"\"\"\n╔════════════════════════════════════════════════════════════════════════════════════════════════════════════════╗\n║                                        EXECUTIVE SUMMARY METRICS                                                ║\n╠════════════════════════════════════════════════════════════════════════════════════════════════════════════════╣\n║                                                                                                                  ║\n║    📊 Total Transactions: {(total_enrollments + total_biometric + total_demographic):,}                                                                   ║\n║                                                                                                                  ║\n║    🗓️  Analysis Period: March 2025 - December 2025 ({(df_enroll['date'].max() - df_enroll['date'].min()).days} days)                                                         ║\n║                                                                                                                  ║\n║    🗺️  Geographic Coverage: {df_enroll['state'].nunique()} States/UTs | {df_enroll['district'].nunique()} Districts | {df_enroll['pincode'].nunique():,} Pincodes                                     ║\n║                                                                                                                  ║\n║    📈 Daily Average: {mean_daily:,.0f} enrollments | Peak Detection: {len(high_anomalies)} high-volume days identified                                   ║\n║                                                                                                                  ║\n║    👥 Dominant Segment: Adults (18+) at {age_18_pct:.1f}% of total enrollments                                                              ║\n║                                                                                                                  ║\n╚════════════════════════════════════════════════════════════════════════════════════════════════════════════════╝\n\"\"\"\nax6.text(0.5, 0.5, summary_text, transform=ax6.transAxes, fontsize=10, \n         verticalalignment='center', horizontalalignment='center', \n         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='#f0f0f0', alpha=0.8))\n\nplt.suptitle('AADHAAR DATA ANALYSIS: FINAL SUMMARY DASHBOARD', \n             fontsize=18, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.savefig('aadhaar_summary_dashboard.png', dpi=150, bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\\n✅ Summary dashboard saved as 'aadhaar_summary_dashboard.png'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 7. Conclusion\n\n### Summary\n\nThis comprehensive analysis of **5+ million Aadhaar records** spanning enrollment, biometric updates, and demographic updates has revealed significant insights into India's digital identity ecosystem.\n\n### Key Takeaways\n\n1. **Scale**: The Aadhaar system successfully processes millions of transactions across all 36 states/UTs\n2. **Geographic Patterns**: Significant state-level variation exists, with larger states dominating absolute numbers\n3. **Temporal Insights**: Strong weekday preference suggests opportunities for weekend service expansion\n4. **Age Demographics**: Adult enrollments dominate, indicating ongoing coverage expansion\n5. **Anomaly Patterns**: Statistical methods successfully identify unusual activity for operational monitoring\n\n### Impact & Applicability\n\nThe findings from this analysis can support:\n- **Policy Makers**: Resource allocation and service coverage decisions\n- **Operations Teams**: Staff scheduling and capacity planning\n- **Quality Assurance**: Anomaly detection and data quality monitoring\n- **Strategic Planning**: Long-term enrollment target setting\n\n### Future Work\n\n- Integration with population census data for penetration analysis\n- Machine learning models for demand forecasting\n- Geographic clustering for optimal center placement\n- Real-time dashboard development for operational monitoring\n\n---\n\n**UIDAI Hackathon 2025**  \n*\"Unlocking Societal Trends in Aadhaar Enrolment and Updates\"*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}